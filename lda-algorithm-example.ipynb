{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3756201,"sourceType":"datasetVersion","datasetId":551982}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This project is to understand the \"Topic Modeling\" concept with LDA(Latent Dirichlet Allocation) algorithm.This algorithm is used to classify text in a document to a particular topic.","metadata":{}},{"cell_type":"code","source":"#import required libraries.\nimport os\nimport re\nimport numpy as np \nimport pandas as pd \nimport json\nfrom pprint import pprint\nimport random\nimport string \n\n#To split the text into tokens or words.\nfrom nltk import word_tokenize\n\n#Stopwords are --> 'a','the'...etc\nfrom nltk.corpus import stopwords\n\n#maps words in document to unique integer IDs.\nfrom gensim.corpora import Dictionary\n\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_colwidth', 100)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-24T12:59:51.945813Z","iopub.execute_input":"2024-07-24T12:59:51.946190Z","iopub.status.idle":"2024-07-24T12:59:51.953001Z","shell.execute_reply.started":"2024-07-24T12:59:51.946157Z","shell.execute_reply":"2024-07-24T12:59:51.951757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"documents_dir='../input/CORD-19-research-challenge/document_parses/pdf_json/'\nfilenames = os.listdir(documents_dir)\nprint(\"Number of documents :\", len(filenames))","metadata":{"execution":{"iopub.status.busy":"2024-07-24T12:59:54.646885Z","iopub.execute_input":"2024-07-24T12:59:54.647280Z","iopub.status.idle":"2024-07-24T12:59:55.187274Z","shell.execute_reply.started":"2024-07-24T12:59:54.647247Z","shell.execute_reply":"2024-07-24T12:59:55.186243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.shuffle(filenames)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T12:59:59.854716Z","iopub.execute_input":"2024-07-24T12:59:59.855101Z","iopub.status.idle":"2024-07-24T13:00:00.172298Z","shell.execute_reply.started":"2024-07-24T12:59:59.855069Z","shell.execute_reply":"2024-07-24T13:00:00.171159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file = json.load(open('../input/CORD-19-research-challenge/document_parses/pdf_json/0000028b5cc154f68b8a269f6578f21e31f62977.json', 'rb'))","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:00:00.861957Z","iopub.execute_input":"2024-07-24T13:00:00.862438Z","iopub.status.idle":"2024-07-24T13:00:00.869287Z","shell.execute_reply.started":"2024-07-24T13:00:00.862397Z","shell.execute_reply":"2024-07-24T13:00:00.868221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pprint(file[\"metadata\"][\"title\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:00:01.706965Z","iopub.execute_input":"2024-07-24T13:00:01.707395Z","iopub.status.idle":"2024-07-24T13:00:01.712501Z","shell.execute_reply.started":"2024-07-24T13:00:01.707360Z","shell.execute_reply":"2024-07-24T13:00:01.711414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step 1 : Data Cleaning**","metadata":{}},{"cell_type":"code","source":"#function to clean(preprocess) the text\ndef clean(text):\n    text = str(text).lower()\n    #To remove '[] brackets'.\n    text = re.sub(r'\\[.*?\\]', '', text)\n    #To remove '() paranthesis'.\n    text = re.sub(r'\\(.*?\\)', '', text)\n    #To remove 'empty spaces 1 or more than'.\n    text = re.sub(r\"\\s+\", \" \", text)\n    #To remove 'alphanumeric and digits'\n    text = re.sub(r'\\w*\\d\\w*', '', text)\n    #To remove ellipsis(which are '...')\n    text = re.sub(r\"\\w+â€¦|â€¦\", \"\", text) \n    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n    return text\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:00:09.702855Z","iopub.execute_input":"2024-07-24T13:00:09.703366Z","iopub.status.idle":"2024-07-24T13:00:09.710932Z","shell.execute_reply.started":"2024-07-24T13:00:09.703310Z","shell.execute_reply":"2024-07-24T13:00:09.709557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to remove and tokenize the text.\ndef remove_stopwords_and_tokenize(text):\n    my_stopwords = set(stopwords.words(\"english\"))\n    tokens = word_tokenize(text)  # tokenize \n    tokens = [t for t in tokens if not t in my_stopwords]  # Remove stopwords\n    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n    return tokens\n","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:00:12.184841Z","iopub.execute_input":"2024-07-24T13:00:12.185226Z","iopub.status.idle":"2024-07-24T13:00:12.192126Z","shell.execute_reply.started":"2024-07-24T13:00:12.185196Z","shell.execute_reply":"2024-07-24T13:00:12.190705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#parsing the text\ndef parse_body_text(body_text):\n    body =\"\"\n    for item in body_text:\n        body += item[\"section\"]\n        body += \"\\n\\n\"\n        body += item[\"text\"]\n        body += \"\\n\\n\"\n    body=clean(body)\n    tokens=remove_stopwords_and_tokenize(body)\n    return body,tokens\n","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:00:14.196198Z","iopub.execute_input":"2024-07-24T13:00:14.196567Z","iopub.status.idle":"2024-07-24T13:00:14.203104Z","shell.execute_reply.started":"2024-07-24T13:00:14.196538Z","shell.execute_reply":"2024-07-24T13:00:14.201643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_text = []\nall_tokens=[]\nall_titles=[]\nfor i,filename in enumerate(filenames[:1000]):\n    filepath = documents_dir + filename\n    file = json.load(open(filepath, 'rb'))\n    text,tokens=parse_body_text(file[\"body_text\"])\n    all_text.append(text)\n    all_tokens.append(tokens)\n    all_titles.append(file[\"metadata\"][\"title\"])\n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:00:17.048477Z","iopub.execute_input":"2024-07-24T13:00:17.048861Z","iopub.status.idle":"2024-07-24T13:00:54.844831Z","shell.execute_reply.started":"2024-07-24T13:00:17.048830Z","shell.execute_reply":"2024-07-24T13:00:54.843652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Arranging the extracted data in Dataframe\ndata = pd.DataFrame()\ndata['text'] = all_text\ndata['tokens'] = all_tokens\ndata['doc_id'] = filenames[:1000]\ndata['title'] = all_titles\ndel all_text,all_tokens,all_titles\n\nif not data['tokens'].empty and all(data['tokens']):\n    print(\"Tokens are populated correctly!\")\nelse:\n    print(\"Tokens are empty!!\")","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:00:58.074298Z","iopub.execute_input":"2024-07-24T13:00:58.074705Z","iopub.status.idle":"2024-07-24T13:00:58.092617Z","shell.execute_reply.started":"2024-07-24T13:00:58.074672Z","shell.execute_reply":"2024-07-24T13:00:58.091374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(2)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:01:08.495822Z","iopub.execute_input":"2024-07-24T13:01:08.496763Z","iopub.status.idle":"2024-07-24T13:01:08.515874Z","shell.execute_reply.started":"2024-07-24T13:01:08.496725Z","shell.execute_reply":"2024-07-24T13:01:08.514767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step 2 : Apply LDA model**","metadata":{}},{"cell_type":"code","source":"#creating a dictionary representation of the documents\ndictionary = Dictionary(data[\"tokens\"])\n\n#Filter out the words that occur less than 20 documents, or than 50% of the documents.\ndictionary.filter_extremes(no_below = 20, no_above = 0.5)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:01:28.201484Z","iopub.execute_input":"2024-07-24T13:01:28.201873Z","iopub.status.idle":"2024-07-24T13:01:30.566448Z","shell.execute_reply.started":"2024-07-24T13:01:28.201835Z","shell.execute_reply":"2024-07-24T13:01:30.565446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Bag-of-words reperesentation of the documents.\ncorpus = [dictionary.doc2bow(doc) for doc in data[\"tokens\"]]","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:01:33.921674Z","iopub.execute_input":"2024-07-24T13:01:33.922061Z","iopub.status.idle":"2024-07-24T13:01:34.971434Z","shell.execute_reply.started":"2024-07-24T13:01:33.922029Z","shell.execute_reply":"2024-07-24T13:01:34.970230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import LdaModel\n\n#Build a LDA Model\nlda_model = LdaModel(corpus = corpus,id2word=dictionary,num_topics=20,random_state=100,chunksize=200,passes=100)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:01:37.887735Z","iopub.execute_input":"2024-07-24T13:01:37.888116Z","iopub.status.idle":"2024-07-24T13:07:18.223675Z","shell.execute_reply.started":"2024-07-24T13:01:37.888085Z","shell.execute_reply":"2024-07-24T13:07:18.222442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#displaying topics that have been identified by LDA Algorithm.\nlda_model.print_topics()[:5]\n#Each word is assigned with a probability in a topic(This probability defines the importance of the word).\n#'index' --> defines the topic.","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:10:34.406997Z","iopub.execute_input":"2024-07-24T13:10:34.407415Z","iopub.status.idle":"2024-07-24T13:10:34.420374Z","shell.execute_reply.started":"2024-07-24T13:10:34.407379Z","shell.execute_reply":"2024-07-24T13:10:34.418836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Topic distributions for the first document.\nlda_model[corpus][0]","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:14:16.399148Z","iopub.execute_input":"2024-07-24T13:14:16.399545Z","iopub.status.idle":"2024-07-24T13:14:16.410697Z","shell.execute_reply.started":"2024-07-24T13:14:16.399514Z","shell.execute_reply":"2024-07-24T13:14:16.409323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step 4 : Results**","metadata":{}},{"cell_type":"code","source":"#Document - Topic Table\ndef get_document_topic_table(lda_model, corpus, texts=data):\n    #Init output\n    document_topic_df = pd.DataFrame()\n    \n    #Get main topic in each documet\n    for i,row_list in enumerate(lda_model[corpus]):\n        row = sorted(row_list, key=lambda x: (x[1]), reverse=True)\n        topic_num = row[0][0]\n        prop_topic = row[0][1]\n        wp = lda_model.show_topic(topic_num)\n        topic_keywords = \", \".join([word for word, prop in wp])\n        document_topic_df.at[i,'best_topic'] = topic_num\n        document_topic_df.at[i,'prop_topic'] = prop_topic\n        document_topic_df.at[i,'topic_keyboards'] = topic_keywords\n        document_topic_df.at[i,'document_num'] = i\n    return document_topic_df\n\ndocument_topic_df = get_document_topic_table(lda_model=lda_model, corpus=corpus, texts=data[\"tokens\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:56:56.778471Z","iopub.execute_input":"2024-07-24T13:56:56.779454Z","iopub.status.idle":"2024-07-24T13:57:00.650812Z","shell.execute_reply.started":"2024-07-24T13:56:56.779417Z","shell.execute_reply":"2024-07-24T13:57:00.649621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"document_topic_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:57:28.128782Z","iopub.execute_input":"2024-07-24T13:57:28.129199Z","iopub.status.idle":"2024-07-24T13:57:28.146426Z","shell.execute_reply.started":"2024-07-24T13:57:28.129168Z","shell.execute_reply":"2024-07-24T13:57:28.145165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Recommend 'k' Topics**","metadata":{}},{"cell_type":"code","source":"def get_topic_id(doc_id):\n    for i,row in data.iterrows():\n        if(row[\"doc_id\"]==doc_id):\n            #print(document_topic_df[\"best_topic\"][i])\n            return document_topic_df[\"best_topic\"][i]\n    return -1\n\ndef get_matching_topics_docs(topic_id):\n    matched_topics=[]\n    for i,row in document_topic_df.iterrows():\n        \n        if(row[\"best_topic\"]==topic_id):\n            topic_prop_doc=(topic_id,row[\"prop_topic\"],i)\n            matched_topics.append(topic_prop_doc)\n        \n    return matched_topics\n    \ndef get_top_k_topics(matched_topics,k):\n    top_k=sorted(matched_topics, key=lambda x: [x[1]], reverse=True)\n    print(top_k[:k])\n    k_topics_df=pd.DataFrame(columns=[\"doc_id\",\"topic_id\",\"topic_prop\",\"title\"])\n    i=0\n    for topic_id,topic_prop,doc_num in top_k[:k]:\n        k_topics_df.at[i,'doc_id']=data[\"doc_id\"][doc_num]\n        k_topics_df.at[i,'topic_id']=topic_id\n        k_topics_df.at[i,'topic_prop']=topic_prop\n        k_topics_df.at[i,'title']=data[\"title\"][doc_num]\n        i+=1\n    return k_topics_df\n\ndef recommend_k_topics(doc_id,k):\n    topic_id=get_topic_id(doc_id)\n    if(topic_id!=-1):\n        matched_topics=get_matching_topics_docs(topic_id) \n        return get_top_k_topics(matched_topics,k)\n    \n    \nk_topics_df=recommend_k_topics('328401206bf2e3657e352ad5c5a2e566cc09736d.json',5)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:59:29.459175Z","iopub.execute_input":"2024-07-24T13:59:29.459582Z","iopub.status.idle":"2024-07-24T13:59:29.530507Z","shell.execute_reply.started":"2024-07-24T13:59:29.459553Z","shell.execute_reply":"2024-07-24T13:59:29.529524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_topics_df","metadata":{"execution":{"iopub.status.busy":"2024-07-24T13:59:36.723588Z","iopub.execute_input":"2024-07-24T13:59:36.723947Z","iopub.status.idle":"2024-07-24T13:59:36.728711Z","shell.execute_reply.started":"2024-07-24T13:59:36.723919Z","shell.execute_reply":"2024-07-24T13:59:36.727623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}